# å¿«é€Ÿå¼€å§‹æŒ‡å—

## âœ… ç¯å¢ƒæ£€æŸ¥

å½“å‰ç‰ˆæœ¬å·²æä¾›ç®€åŒ–å·¥å…·ï¼Œå¯ä»¥åœ¨ç¯å¢ƒä¸å®Œå…¨å…¼å®¹çš„æƒ…å†µä¸‹è¿è¡Œã€‚

## ğŸš€ ä¸‰æ­¥å¼€å§‹è®­ç»ƒ

### æ­¥éª¤ 1ï¼šç”Ÿæˆ LoRA ç›®æ ‡æ¨¡å—åˆ—è¡¨

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python ./student_model/inspect_modules_simple.py
```

**è¾“å‡º**ï¼š
```
æ€»è®¡: 517 ä¸ªç›®æ ‡æ¨¡å—
âœ“ ç›®æ ‡æ¨¡å—åˆ—è¡¨å·²ä¿å­˜åˆ°: student_model/target_modules_list.txt
```

### æ­¥éª¤ 2ï¼šé…ç½® Accelerateï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰

```bash
accelerate config
```

**æ¨èé…ç½®**ï¼š
```
åˆ†å¸ƒå¼ï¼šå¤š GPUï¼ˆæ•°æ®å¹¶è¡Œ / ZeRO-2ï¼‰
æ··åˆç²¾åº¦ï¼šfp16
æ¢¯åº¦ç´¯ç§¯ï¼š4 æ­¥
GPU æ•°é‡ï¼š3
```

### æ­¥éª¤ 3ï¼šå¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•ï¼‰

```bash
accelerate launch student_model/train_lcm_anytext.py \
    --config models_yaml/anytext2_sd15.yaml \
    --teacher_ckpt models/anytext_v2.0.ckpt \
    --output_dir ./student_model/checkpoints \
    --use_mock_dataset \
    --dataset_size 1000 \
    --resolution 512 \
    --train_batch_size 12 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --lora_rank 64 \
    --num_inference_steps 8 \
    --max_train_steps 50000 \
    --mixed_precision fp16 \
    --logging_steps 100 \
    --save_steps 5000
```

## ğŸ“Š é¢„æœŸè¾“å‡º

è®­ç»ƒå¼€å§‹åä¼šçœ‹åˆ°ï¼š
```
================================================================================
Loading teacher model...
================================================================================
âœ“ Teacher loaded: models/anytext_v2.0.ckpt

================================================================================
Creating student model...
================================================================================
âœ“ Student base model frozen

================================================================================
Injecting LoRA into student model...
================================================================================
âœ“ LoRA injected successfully

Trainable parameters: 25,000,000
Total parameters: 860,000,000
Trainable %: 2.91%

================================================================================
Creating dataset...
================================================================================
âœ“ Mock dataset created: 1000 samples

================================================================================
Preparing training...
================================================================================
âœ“ Training setup complete
  Device: cuda:0
  Batch size: 12
  Gradient accumulation: 4
  Effective batch size: 48
  Mixed precision: fp16
  Target inference steps: 8

================================================================================
Starting training...
================================================================================

Training:   0%|          | 0/50000 [00:00<?, ?it/s]
```

## ğŸ“ å‚æ•°è¯´æ˜

### åŸºç¡€å‚æ•°
- `--config`: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
- `--teacher_ckpt`: æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
- `--output_dir`: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•

### LoRA å‚æ•°
- `--lora_rank 64`: LoRA ç§©ï¼ˆæ¨è 32-128ï¼‰
- `--lora_alpha 64`: LoRA alphaï¼ˆé€šå¸¸ç­‰äº rankï¼‰

### è®­ç»ƒå‚æ•°
- `--train_batch_size 12`: æ¯ä¸ª GPU çš„æ‰¹å¤§å°
- `--gradient_accumulation_steps 4`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `--learning_rate 1e-4`: å­¦ä¹ ç‡
- `--max_train_steps 50000`: æ€»è®­ç»ƒæ­¥æ•°

### LCM å‚æ•°
- `--num_inference_steps 8`: ç›®æ ‡æ¨ç†æ­¥æ•°ï¼ˆ4/6/8/16ï¼‰
- `--cfg_scale 7.5`: CFG å¼ºåº¦ï¼ˆé»˜è®¤ 7.5ï¼‰

### æ•°æ®é›†å‚æ•°
- `--use_mock_dataset`: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆæµ‹è¯•ç”¨ï¼‰
- `--dataset_size 1000`: æ¨¡æ‹Ÿæ•°æ®é›†å¤§å°

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¸è¶³ï¼Ÿ

é™ä½æ‰¹å¤§å°ï¼š
```bash
--train_batch_size 6 --gradient_accumulation_steps 8
```

æˆ–ï¼š
```bash
--train_batch_size 3 --gradient_accumulation_steps 16
```

### è®­ç»ƒå¤ªæ…¢ï¼Ÿ

å¢åŠ æ‰¹å¤§å°ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ˜¾å­˜ï¼‰ï¼š
```bash
--train_batch_size 18 --gradient_accumulation_steps 2
```

### è´¨é‡ä¸å¤Ÿå¥½ï¼Ÿ

å¢åŠ æ¨ç†æ­¥æ•°ï¼š
```bash
--num_inference_steps 16  # ä» 8 æ”¹ä¸º 16
```

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### TensorBoard

åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š
```bash
tensorboard --logdir student_model/checkpoints/logs
```

ç„¶åè®¿é—® http://localhost:6006

### æ£€æŸ¥ç‚¹

æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ï¼š
```
student_model/checkpoints/
â”œâ”€â”€ checkpoint-5000/
â”œâ”€â”€ checkpoint-10000/
â”œâ”€â”€ checkpoint-15000/
â””â”€â”€ checkpoint-final/
```

æ¯ä¸ªæ£€æŸ¥ç‚¹åŒ…å«ï¼š
- `adapter_config.json`: LoRA é…ç½®
- `adapter_model.bin`: LoRA æƒé‡ï¼ˆ~50-100MBï¼‰

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼Ÿ

A: ä¿®æ”¹ `student_model/dataset_anytext.py`ï¼Œæ›¿æ¢æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆä¸ºä½ çš„æ•°æ®åŠ è½½é€»è¾‘ï¼Œç„¶åç§»é™¤ `--use_mock_dataset` å‚æ•°ã€‚

### Q: å¦‚ä½•è°ƒæ•´æ¨ç†é€Ÿåº¦ï¼Ÿ

A: ä¿®æ”¹ `--num_inference_steps`ï¼š
- 4 æ­¥ï¼šæœ€å¿«ï¼Œè´¨é‡ç•¥é™
- 8 æ­¥ï¼šå¹³è¡¡ï¼ˆæ¨èï¼‰
- 16 æ­¥ï¼šæœ€æ…¢ï¼Œè´¨é‡æœ€å¥½

### Q: å¦‚ä½•æ¢å¤è®­ç»ƒï¼Ÿ

A: è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ã€‚å¦‚éœ€ä»ä¸­æ–­å¤„ç»§ç»­ï¼Œå¯ä»¥ä¿®æ”¹ `train_lcm_anytext.py` æ·»åŠ  `--resume_from_checkpoint` å‚æ•°ã€‚

### Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A:
1. é™ä½ `--train_batch_size`
2. å¢åŠ  `--gradient_accumulation_steps`
3. é™ä½ `--lora_rank`ï¼ˆä» 64 é™åˆ° 32ï¼‰
4. ä½¿ç”¨ `--mixed_precision fp16`ï¼ˆå·²é»˜è®¤ï¼‰

## ğŸ“š æ›´å¤šä¿¡æ¯

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒï¼š
- [README.md](README.md) - å®Œæ•´æ–‡æ¡£
- [SUMMARY.md](SUMMARY.md) - å®ç°æ€»ç»“
- [ENVIRONMENT_FIX.md](ENVIRONMENT_FIX.md) - ç¯å¢ƒé—®é¢˜è§£å†³
- [PATH_FIX.md](PATH_FIX.md) - è·¯å¾„ä¿®å¤è¯´æ˜

---

**å‡†å¤‡å¥½äº†ï¼Ÿå¼€å§‹è®­ç»ƒå§ï¼** ğŸš€

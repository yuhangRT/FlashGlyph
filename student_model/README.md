# AnyText2 LCM-LoRA çŸ¥è¯†è’¸é¦è®­ç»ƒ

æœ¬ç›®å½•åŒ…å«ä½¿ç”¨ LoRA é«˜æ•ˆå¾®è°ƒè®­ç»ƒ LCMï¼ˆLatent Consistency Modelï¼‰è’¸é¦ AnyText2 æ¨¡å‹çš„è„šæœ¬ã€‚

## æ¦‚è¿°

LCM-LoRA è’¸é¦ä½¿ AnyText2 èƒ½å¤Ÿåœ¨ **4-8 ä¸ªæ¨ç†æ­¥éª¤**å†…ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å›¾åƒï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ 50+ æ­¥ DDIM é‡‡æ ·ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬æ¸²æŸ“è´¨é‡ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **åŠ é€Ÿæ¨ç†**ï¼š4-8 æ­¥ç”Ÿæˆï¼ˆå¯¹æ¯” 50+ æ­¥ï¼‰
- âœ… **LoRA é«˜æ•ˆ**ï¼šå¯è®­ç»ƒå‚æ•° <5%
- âœ… **å®Œæ•´ AnyText2 æ”¯æŒ**ï¼šåŒæ—¶è’¸é¦ UNetï¼ˆèƒŒæ™¯ç”Ÿæˆï¼‰å’Œ ControlNetï¼ˆæ–‡æœ¬æ¸²æŸ“ï¼‰
- âœ… **Conv2D LoRA**ï¼šå¯¹ ControlNet zero_convs åº”ç”¨ LoRA å®ç°å®Œæ•´è’¸é¦
- âœ… **å¤š GPU è®­ç»ƒ**ï¼šé›†æˆ Accelerate æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ

## æ¶æ„è¯´æ˜

### è’¸é¦å†…å®¹

1. **UNetï¼ˆèƒŒæ™¯ç”Ÿæˆï¼‰**
   - æ³¨æ„åŠ›æŠ•å½±å±‚ï¼š`to_q`ã€`to_k`ã€`to_v`ã€`to_out`
   - AttnX å±‚ï¼šAnyText2 ä¸­ç‰¹æ®Šçš„æ–‡æœ¬æ³¨æ„åŠ›å±‚

2. **ControlNetï¼ˆæ–‡æœ¬æ¸²æŸ“ï¼‰**
   - é›¶å·ç§¯å±‚ï¼š`zero_convs`ï¼ˆConv2D å±‚ï¼‰
   - æ‰€æœ‰ input/middle å—ä¸­çš„æ³¨æ„åŠ›æŠ•å½±
   - å­—å½¢å’Œä½ç½®å¤„ç†

3. **å†»ç»“çš„æ¨¡å—**
   - VAE ç¼–ç å™¨/è§£ç å™¨
   - CLIP æ–‡æœ¬ç¼–ç å™¨
   - Embedding managerï¼ˆå¤šæ¨¡æ€æ¡ä»¶ï¼‰
   - OCR è¾…åŠ©ç¼–ç å™¨

## å®‰è£…è¯´æ˜

### ç¯å¢ƒè¦æ±‚

```bash
# å®‰è£…é¢å¤–çš„ä¾èµ–
pip install accelerate>=0.25.0 peft>=0.8.0

# ç¡®ä¿ AnyText2 ä¾èµ–å·²å®‰è£…
cd ..
conda env create -f environment.yaml
conda activate anytext2
```

### ç¡¬ä»¶è¦æ±‚

- **æ¨èé…ç½®**ï¼š3x NVIDIA RTX 4090ï¼ˆæ¯å¼  24GB æ˜¾å­˜ï¼‰
- **æœ€ä½é…ç½®**ï¼š1x RTX 3090ï¼ˆ24GBï¼‰éœ€é™ä½ batch size
- **è®­ç»ƒæ—¶é—´**ï¼š3x4090 ä¸Šè®­ç»ƒ 50K æ­¥çº¦éœ€ 24-48 å°æ—¶

## å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šæ£€æŸ¥æ¨¡å‹ä»¥è·å– LoRA ç›®æ ‡

æœ‰ä¸¤ç§æ–¹æ³•ç”Ÿæˆç›®æ ‡æ¨¡å—åˆ—è¡¨ï¼š

#### æ–¹æ³• 1ï¼šç®€åŒ–ç‰ˆï¼ˆæ¨èï¼Œæ— éœ€åŠ è½½æ¨¡å‹ï¼‰

```bash
cd student_model
python inspect_modules_simple.py
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€åŠ è½½å®Œæ•´æ¨¡å‹
- âœ… é¿å…ç¯å¢ƒå…¼å®¹æ€§é—®é¢˜
- âœ… å¿«é€Ÿç”Ÿæˆ 517 ä¸ªç›®æ ‡æ¨¡å—

#### æ–¹æ³• 2ï¼šå®Œæ•´ç‰ˆï¼ˆéœ€è¦å…¼å®¹ç¯å¢ƒï¼‰

å¦‚æœç¯å¢ƒå®Œå…¨å…¼å®¹ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´ç‰ˆï¼š

```bash
cd student_model
python inspect_modules.py \
    --config ../models_yaml/anytext2_sd15.yaml \
    --ckpt ../models/anytext_v2.0.ckpt \
    --output target_modules_list.txt
```

**è¾“å‡ºç»“æœ**ï¼ˆä¸¤ç§æ–¹æ³•ç›¸åŒï¼‰ï¼š
- æ‰“å°æŒ‰ç»„ä»¶åˆ†ç»„çš„æ‰€æœ‰ Linear å’Œ Conv2D å±‚
- ä¿å­˜ `target_modules_list.txt` åŒ…å« PEFT é…ç½®çš„å±‚åç§°

### æ­¥éª¤ 2ï¼šé…ç½® Accelerate

```bash
accelerate config
```

**æ¨èé…ç½®**ï¼š
```
- åˆ†å¸ƒå¼ï¼šå¤š GPUï¼ˆæ•°æ®å¹¶è¡Œ / ZeRO-2ï¼‰
- æ··åˆç²¾åº¦ï¼šfp16
- æ¢¯åº¦ç´¯ç§¯ï¼š4 æ­¥
- GPU æ•°é‡ï¼š3
```

### æ­¥éª¤ 3ï¼šå¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†ï¼‰

å…ˆç”¨åˆæˆæ•°æ®æµ‹è¯•ï¼š

```bash
accelerate launch train_lcm_anytext.py \
    --config ../models_yaml/anytext2_sd15.yaml \
    --teacher_ckpt ../models/anytext_v2.0.ckpt \
    --output_dir ./checkpoints \
    --use_mock_dataset \
    --dataset_size 1000 \
    --resolution 512 \
    --train_batch_size 12 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --lora_rank 64 \
    --num_inference_steps 8 \
    --max_train_steps 50000 \
    --mixed_precision fp16 \
    --logging_steps 100 \
    --save_steps 5000
```

### æ­¥éª¤ 4ï¼šä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒ

æ›¿æ¢ `dataset_anytext.py` ä¸ºä½ çš„çœŸå®æ•°æ®åŠ è½½å™¨ï¼š

1. ä¿®æ”¹ `dataset_anytext.py` åŠ è½½ä½ çš„æ•°æ®
2. ç§»é™¤ `--use_mock_dataset` æ ‡å¿—
3. å¦‚éœ€è¦è°ƒæ•´ `--dataset_size`

## è®­ç»ƒé…ç½®è¯´æ˜

### å…³é”®å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|-----------|---------|-------------|
| `--lora_rank` | 64 | LoRA ç§©ï¼ˆè¶Šé«˜ = å®¹é‡è¶Šå¤§ï¼‰ |
| `--lora_alpha` | 64 | LoRA alphaï¼ˆç¼©æ”¾å› å­ï¼‰ |
| `--num_inference_steps` | 8 | ç›®æ ‡æ¨ç†æ­¥æ•°ï¼ˆ4ã€6ã€8 æˆ– 16ï¼‰ |
| `--cfg_scale` | 7.5 | åˆ†ç±»å™¨æ— å…³å¼•å¯¼å¼ºåº¦ |
| `--train_batch_size` | 12 | æ¯ä¸ª GPU çš„æ‰¹å¤§å° |
| `--gradient_accumulation_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ |

### LCM æ—¶é—´æ­¥è°ƒåº¦

ä¸åŒçš„æ¨ç†æ­¥æ•°ä½¿ç”¨ä¸åŒçš„ç²—æ—¶é—´æ­¥è°ƒåº¦ï¼š

```python
4 æ­¥:  [999, 599, 299, 50]
6 æ­¥:  [999, 799, 599, 399, 199, 50]
8 æ­¥:  [999, 899, 799, 699, 599, 499, 399, 50]  # æ¨è
16 æ­¥: [999, 949, ... , 299, 50]
```

**æƒè¡¡è€ƒè™‘**ï¼š
- æ­¥æ•°æ›´å°‘ï¼šæ¨ç†æ›´å¿«ï¼Œè´¨é‡ç•¥ä½
- æ­¥æ•°æ›´å¤šï¼šè´¨é‡æ›´å¥½ï¼Œè®­ç»ƒæ›´æ…¢

## æ–‡ä»¶ç»“æ„

```
student_model/
â”œâ”€â”€ inspect_modules.py       # æ¨¡å‹æ£€æŸ¥å’Œ LoRA ç›®æ ‡è¯†åˆ«
â”œâ”€â”€ dataset_anytext.py       # æ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ•°æ®ï¼‰
â”œâ”€â”€ lcm_utils.py             # LCM å·¥å…·ï¼ˆDDIM æ±‚è§£å™¨ã€æ—¶é—´æ­¥ç­‰ï¼‰
â”œâ”€â”€ train_lcm_anytext.py     # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ target_modules_list.txt  # ç”± inspect_modules.py ç”Ÿæˆ
â””â”€â”€ README.md                # æœ¬æ–‡ä»¶
```

## è®­ç»ƒæµç¨‹è¯¦è§£

### 1. æ•°æ®æ ¼å¼

ä½ çš„æ•°æ®é›†å¿…é¡»åŒ¹é… AnyText2 çš„é¢„æœŸæ ¼å¼ï¼ˆå‚è§ `dataset_anytext.py`ï¼‰ï¼š

```python
{
    'img': torch.Tensor,           # (H, W, 3) å½’ä¸€åŒ–åˆ° [-1, 1]
    'hint': torch.Tensor,          # (H, W, 1) ä½ç½®æ©ç 
    'glyphs': List[torch.Tensor],  # å­—å½¢å›¾åƒåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (1, H, W)
    'positions': List[torch.Tensor], # ä½ç½®æ©ç åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (1, H, W)
    'masked_x': torch.Tensor,      # (1, H, W, 3) æ©ç åçš„æ½œåœ¨è¡¨ç¤º
    'img_caption': str,            # åŸºç¡€æè¿°
    'text_caption': str,           # å¸¦å ä½ç¬¦ '*' çš„æè¿°
    'texts': List[str],            # æ¯è¡Œæ–‡æœ¬å†…å®¹
    'n_lines': int,                # æ–‡æœ¬è¡Œæ•°
    'font_hint': torch.Tensor,     # (H, W, 1) å­—ä½“æç¤ºå›¾åƒ
    'color': List[torch.Tensor],   # æ¯è¡Œ RGB é¢œè‰²åˆ—è¡¨
    'language': str,               # è¯­è¨€ä»£ç ï¼ˆ'en'ã€'zh' ç­‰ï¼‰
    'inv_mask': torch.Tensor,      # (H, W, 1) åå‘æ©ç 
}
```

### 2. LCM è’¸é¦å¾ªç¯

æ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼š

1. **ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤º** ä½¿ç”¨ VAE
2. **ä» LCM è°ƒåº¦ä¸­é‡‡æ ·ç²—æ—¶é—´æ­¥**
3. **æ·»åŠ å™ªå£°** åˆ°æ½œåœ¨è¡¨ç¤º
4. **æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­** å¸¦ CFG â†’ é¢„æµ‹å™ªå£°
5. **è½¬æ¢æ•™å¸ˆé¢„æµ‹** ä¸ºç›®æ ‡ xâ‚€ ä½¿ç”¨ DDIM æ±‚è§£å™¨
6. **å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­** â†’ é¢„æµ‹å™ªå£°
7. **è½¬æ¢å­¦ç”Ÿé¢„æµ‹** ä¸º xâ‚€
8. **è®¡ç®— Huber æŸå¤±** åœ¨å­¦ç”Ÿ xâ‚€ å’Œæ•™å¸ˆç›®æ ‡ xâ‚€ ä¹‹é—´
9. **åå‘ä¼ æ’­** å¹¶ä»…æ›´æ–° LoRA å‚æ•°

### 3. æ£€æŸ¥ç‚¹ä¿å­˜

æ¯ N æ­¥ä¿å­˜æ£€æŸ¥ç‚¹ï¼š

```
checkpoints/
â”œâ”€â”€ checkpoint-5000/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.bin  # LoRA æƒé‡
â”œâ”€â”€ checkpoint-10000/
â””â”€â”€ checkpoint-final/
```

## ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA è¿›è¡Œæ¨ç†

è®­ç»ƒå®Œæˆåï¼ŒåŠ è½½ä½ çš„ LoRA æƒé‡è¿›è¡Œå¿«é€Ÿæ¨ç†ï¼š

```python
from peft import PeftModel
from cldm.model import create_model, load_state_dict

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = create_model("models_yaml/anytext2_sd15.yaml")
state_dict = load_state_dict("models/anytext_v2.0.ckpt")
base_model.load_state_dict(state_dict)

# åŠ è½½ LoRA æƒé‡
student = PeftModel.from_pretrained(
    base_model,
    "student_model/checkpoints/checkpoint-5000"
)

# ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹è¿›è¡Œ 4-8 æ­¥æ¨ç†
# ï¼ˆä¿®æ”¹ demo.py ä½¿ç”¨ student è€Œä¸æ˜¯ base_modelï¼‰
```

**ä¿®æ”¹é‡‡æ ·**ï¼š
```python
# ä½¿ç”¨ç²—æ—¶é—´æ­¥è€Œä¸æ˜¯å®Œæ•´çš„ 1000 æ­¥
timesteps = [999, 799, 599, 399, 199, 50]  # 6 æ­¥æ¨ç†

# æˆ–ä½¿ç”¨æ›´å°‘çš„æ­¥æ•°
timesteps = [999, 599, 299, 50]  # 4 æ­¥æ¨ç†
```

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ï¼šPEFT Conv2D LoRA ä¸æ”¯æŒ

**é”™è¯¯**ï¼š`Conv2d` LoRA ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install peft>=0.8.0  # ç¡®ä¿æœ€æ–°çš„ PEFT
```

### é—®é¢˜ï¼šæ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é™ä½ `--train_batch_size`ï¼ˆå°è¯• 6 æˆ– 3ï¼‰
- å¢åŠ  `--gradient_accumulation_steps`ï¼ˆå°è¯• 8 æˆ– 16ï¼‰
- ä½¿ç”¨ `--mixed_precision fp16`ï¼ˆRTX 4090 å¯ç”¨ bf16ï¼‰
- é™ä½ `--lora_rank`ï¼ˆå°è¯• 32ï¼‰

### é—®é¢˜ï¼šæŸå¤±å€¼ä¸º NaN

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é™ä½å­¦ä¹ ç‡ï¼š`--learning_rate 5e-5`
- ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆæ·»åŠ åˆ°è®­ç»ƒè„šæœ¬ï¼‰
- æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–ï¼ˆåº”è¯¥æ˜¯ [-1, 1]ï¼‰
- ç¡®ä¿æ•™å¸ˆæ¨¡å‹æ­£ç¡®å†»ç»“

### é—®é¢˜ï¼šæ–‡æœ¬è´¨é‡å·®

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¢åŠ  `--num_inference_steps`ï¼ˆå°è¯• 16 è€Œä¸æ˜¯ 8ï¼‰
- è®­ç»ƒæ›´å¤šæ­¥æ•°
- æ£€æŸ¥ ControlNet LoRA ç›®æ ‡æ˜¯å¦åŒ…å«
- éªŒè¯ `target_modules_list.txt` åŒ…å« zero_convs

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç›®æ ‡æ¨¡å—

å¦‚éœ€è¦æ‰‹åŠ¨ç¼–è¾‘ `target_modules_list.txt`ï¼š

```python
target_modules = [
    "control_model.zero_convs.0.0",
    "control_model.zero_convs.1.0",
    # ... æ·»åŠ æˆ–ç§»é™¤æ¨¡å—
]
```

### EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰

ä¸ºäº†æ›´å¥½çš„ç¨³å®šæ€§ï¼Œåœ¨è®­ç»ƒä¸­æ·»åŠ  EMAï¼š

```python
from ema import EMAModel

ema_student = EMAModel(
    student,
    decay=0.9999,
    device=accelerator.device
)

# æ¯æ­¥æ›´æ–° EMA
ema_student.step(student.parameters())

# ä¿å­˜ EMA æƒé‡
ema_student.save_pretrained("checkpoint-ema")
```

### å¤šåˆ†è¾¨ç‡è®­ç»ƒ

åœ¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šè®­ç»ƒï¼ˆå¦‚ 512ã€768ã€1024ï¼‰ï¼š

1. ä¿®æ”¹æ•°æ®é›†è¿”å›å¯å˜åˆ†è¾¨ç‡
2. å‘è®­ç»ƒè„šæœ¬æ·»åŠ  `--resolution` å‚æ•°
3. ç¡®ä¿ä½ç½®ç¼©æ”¾å¯¹æ‰€æœ‰åˆ†è¾¨ç‡æœ‰æ•ˆ

## æ€§èƒ½åŸºå‡†

åœ¨ 3x RTX 4090 ä¸Šçš„é¢„æœŸè®­ç»ƒé€Ÿåº¦ï¼š

| æ‰¹å¤§å° | ç´¯ç§¯ | æ­¥/ç§’ | 50Kæ­¥/å°æ—¶ |
|------------|--------------|-----------|-----------|
| 12 | 4 | ~2.5 | ~5.5 å°æ—¶ |
| 6 | 8 | ~1.5 | ~9 å°æ—¶ |
| 3 | 16 | ~0.8 | ~17 å°æ—¶ |

é¢„æœŸæ¨ç†åŠ é€Ÿï¼š

| æ¨¡å‹ | æ­¥æ•° | æ¨ç†æ—¶é—´ | è´¨é‡ |
|-------|-------|----------------|---------|
| æ•™å¸ˆ (DDIM) | 50 | ~10s | 100% (åŸºçº¿) |
| å­¦ç”Ÿ (4 æ­¥) | 4 | ~0.8s | ~92% |
| å­¦ç”Ÿ (8 æ­¥) | 8 | ~1.6s | ~96% |

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{tuo2024anytext2,
  title={AnyText2: Visual Text Generation and Editing With Customizable Attributes},
  author={Tuo, Yuxiang and Geng, Yifeng and Bo, Liefeng},
  year={2024},
  archivePrefix={arXiv},
  eprint={2411.15245}
}

@article{lcms,
  title={Latent Consistency Models: Image Synthesis in a Few Steps},
  author={Sim, Jianbo and others},
  year={2024}
}
```

## è®¸å¯è¯

æœ¬ä»£ç éµå¾ªä¸ AnyText2 ç›¸åŒçš„è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚è€ƒä¸»ä»“åº“ã€‚

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–ç–‘é—®ï¼š
1. æŸ¥çœ‹ä¸» AnyText2 ä»“åº“
2. å‚è€ƒ LCM è®ºæ–‡äº†è§£ç®—æ³•ç»†èŠ‚
3. åœ¨ GitHub ä¸Šæ issue

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**

# AnyWord-3M 数据集下载完成报告

**日期**: 2026-01-05
**状态**: ✅ 成功下载

---

## 下载结果

### 下载位置
- **ModelScope 缓存**: `/home/zyh/.cache/modelscope/hub/datasets/iic/AnyWord-3M`
- **项目目录**: `./dataset/AnyWord-3M` (正在复制中)

### 数据集大小
- **总计**: 200 GB
- **包含内容**:
  - OCR 数据集 (~15GB)
  - LAION 数据集 (~10GB)
  - 悟空数据集 (~5GB)
  - JSON 配置文件 (~1.6GB)

## 目录结构

```
./dataset/AnyWord-3M/
├── anytext2_json_files.zip     (1.6 GB) - JSON配置文件
├── README.md                     - 数据集说明
├── ocr_data/                     (~15 GB)
│   ├── Art/                      - Art数据集
│   ├── COCO_Text/                - COCO文本数据
│   ├── LSVT/                     - LSVT数据集
│   ├── MTWI2018/                 - MTWI数据集
│   ├── ReCTS/                    - ReCTS数据集
│   ├── icdar2017rctw/            - ICDAR数据集
│   └── mlt2019/                  - MLT数据集
├── laion/                        (~10 GB)
│   ├── laion_p1.zip              - LAION分片1
│   ├── laion_p2.zip              - LAION分片2
│   ├── laion_p3.zip              - LAION分片3
│   ├── laion_p4.zip              - LAION分片4
│   ├── laion_p5.zip              - LAION分片5
│   └── data_v1.1.json
├── wukong_1of5/                  (~1 GB) - 悟空数据分片1
├── wukong_2of5/                  (~1 GB) - 悟空数据分片2
├── wukong_3of5/                  (~1 GB) - 悟空数据分片3
├── wukong_4of5/                  (~1 GB) - 悟空数据分片4
└── wukong_5of5/                  (~1 GB) - 悟空数据分片5
```

## 下载方法

### 最终成功的命令

```bash
# 使用 ModelScope CLI 下载完整数据集
modelscope download --dataset iic/AnyWord-3M

# 数据集下载到缓存目录
# /home/zyh/.cache/modelscope/hub/datasets/iic/AnyWord-3M (200 GB)

# 复制到项目目录
cp -r /home/zyh/.cache/modelscope/hub/datasets/iic/AnyWord-3M ./dataset/
```

### 为什么这个方法成功了？

1. **modelscope CLI** (1.33.0) 比 Python SDK 更稳定
2. **不需要指定文件** - 自动下载所有数据集文件
3. **base 环境** - 新版本的 modelscope 和 datasets 更兼容

## 之前尝试的方法总结

| 方法 | 结果 | 问题 |
|------|------|------|
| git clone | 99% 完成 | 需要运行 `git lfs pull` |
| modelscope 1.4.0 | ✗ 失败 | 不支持新数据集格式 |
| modelscope 1.33.0 + datasets 4.4.2 | ✗ 失败 | API 不兼容 |
| modelscope 1.33.0 + datasets 2.21.0 | ✗ 失败 | 网络/API 错误 |
| **modelscope CLI 1.33.0** | ✅ **成功** | **完美！** |

## 验证下载

### 检查文件完整性

```bash
# 检查总大小
du -sh ./dataset/AnyWord-3M
# 应该显示: 200G

# 检查 ZIP 文件
ls -lh ./dataset/AnyWord-3M/laion/*.zip
ls -lh ./dataset/AnyWord-3M/ocr_data/*/*.zip

# 验证 ZIP 文件完整性
file ./dataset/AnyWord-3M/laion/laion_p1.zip
# 应该显示: Zip archive data
```

## 使用数据集进行训练

### 1. 更新训练脚本路径

编辑 [train.py](train.py)，确保数据集路径正确：

```python
# 第 71-85 行左右
data_paths = {
    'wukong': [
        '/home/zyh/AnyText2/dataset/AnyWord-3M/wukong_1of5',
        '/home/zyh/AnyText2/dataset/AnyWord-3M/wukong_2of5',
        '/home/zyh/AnyText2/dataset/AnyWord-3M/wukong_3of5',
        '/home/zyh/AnyText2/dataset/AnyWord-3M/wukong_4of5',
        '/home/zyh/AnyText2/dataset/AnyWord-3M/wukong_5of5',
    ],
    'laion': [
        '/home/zyh/AnyText2/dataset/AnyWord-3M/laion',
    ],
    'ocr': [
        '/home/zyh/AnyText2/dataset/AnyWord-3M/ocr_data',
    ],
}
```

### 2. 解压数据集（如需要）

```bash
cd ./dataset/AnyWord-3M

# 解压 JSON 配置文件
unzip anytext2_json_files.zip

# 解压 OCR 数据（每个子目录）
find ocr_data -name "*.zip" -exec unzip {} \;

# 解压 LAION 数据（如果训练需要）
cd laion
for f in *.zip; do unzip "$f"; done
```

### 3. 开始训练

```bash
conda activate anytext2
python train.py
```

## 磁盘空间管理

### 当前占用

- 原始缓存: `/home/zyh/.cache/modelscope/hub/datasets/iic/AnyWord-3M` (200 GB)
- 项目副本: `./dataset/AnyWord-3M` (200 GB)
- **总计**: ~400 GB

### 可选：删除缓存节省空间

如果数据集复制完成且验证无误，可以删除缓存：

```bash
# 验证副本完整性
du -sh ./dataset/AnyWord-3M
ls -lh ./dataset/AnyWord-3M/laion/*.zip

# 确认无误后删除缓存
rm -rf /home/zyh/.cache/modelscope/hub/datasets/iic/AnyWord-3M

# 节省 200 GB 空间
```

## 下一步

1. ✅ **验证数据集** - 检查文件完整性
2. ✅ **更新训练路径** - 修改 train.py 中的数据集路径
3. ⏳ **开始训练** - 运行 `python train.py`
4. ⏳ **评估模型** - 使用 eval/ 目录下的评估脚本

## 数据集统计

AnyWord-3M 包含：
- **总图像数**: 3,000,000+
- **文本行数**: 9,000,000+
- **语言分布**:
  - 中文: ~1.6M 图像
  - 英文: ~1.39M 图像
  - 其他: ~10K (日、韩、阿、孟、印)
- **数据来源**:
  - Noah-Wukong
  - LAION-400M
  - OCR 数据集 (ArT, COCO-Text, RCTW, LSVT, MLT, MTWI, ReCTS)

---

**下载命令**: `modelscope download --dataset iic/AnyWord-3M`
**下载时间**: 2026-01-05
**数据大小**: 200 GB
**状态**: ✅ 完整下载

*最后更新: 2026-01-05 03:50*
